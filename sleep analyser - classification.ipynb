{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byE3hqDqzRqD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dViCkk_40oBo"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHqWy4KqzD1e",
        "outputId": "ad19a562-4df2-4ea2-97e4-8181801b8a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best Params: {'et__max_depth': 14, 'et__n_estimators': 271, 'final_estimator__lr__C': np.float64(2.541255222477742), 'rf__max_depth': 10, 'rf__n_estimators': 259, 'svm__svc__C': np.float64(5.287906217433661), 'xgb__xgb_model__learning_rate': np.float64(0.22090568766855337), 'xgb__xgb_model__max_depth': 3, 'xgb__xgb_model__n_estimators': 111}\n",
            "Best CV Accuracy: 0.875\n",
            "Test Accuracy: 0.9117647058823529\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Excellent       1.00      0.91      0.95        34\n",
            "        Fair       0.84      0.94      0.89        50\n",
            "        Good       0.89      0.95      0.92        42\n",
            "        Poor       0.97      0.84      0.90        44\n",
            "\n",
            "    accuracy                           0.91       170\n",
            "   macro avg       0.93      0.91      0.92       170\n",
            "weighted avg       0.92      0.91      0.91       170\n",
            "\n",
            "[[31  1  2  0]\n",
            " [ 0 47  2  1]\n",
            " [ 0  2 40  0]\n",
            " [ 0  6  1 37]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from scipy.stats import randint, uniform\n",
        "import pandas as pd\n",
        "\n",
        "file_path = r\"/content/Balanced_Sleep_Quality_Data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop 'Blood Pressure' which was identified as problematic by XGBoost error\n",
        "df.drop(['Blood Pressure'], axis=1, inplace=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Quality of Sleep', axis=1)\n",
        "y = df['Quality of Sleep']\n",
        "\n",
        "# Convert continuous target to discrete categories\n",
        "# You can adjust the bins and labels based on your data and desired categories\n",
        "bins = [0, 4, 6, 8, 10] # Example bins\n",
        "labels = ['Poor', 'Fair', 'Good', 'Excellent'] # Example labels\n",
        "y_classified = pd.cut(y, bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Drop rows where y_classified is NaN (values outside bins)\n",
        "nan_indices = y_classified.isna()\n",
        "X = X[~nan_indices]\n",
        "y_classified = y_classified[~nan_indices]\n",
        "\n",
        "# Encode categorical features\n",
        "label_cols = ['Gender', 'Occupation', 'BMI Category'] # Exclude 'Blood Pressure' as it's dropped\n",
        "for col in label_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classified, test_size=0.2, random_state=42, stratify=y_classified)\n",
        "\n",
        "\n",
        "# Base learners with pipelines for scaling where needed\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('xgb', Pipeline([\n",
        "        ('scaler', StandardScaler()), # Add scaler for XGBoost after encoding\n",
        "        ('xgb_model', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
        "    ])),\n",
        "    ('svm', Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('svc', SVC(probability=True, random_state=42))\n",
        "    ])),\n",
        "    ('et', RandomForestClassifier(random_state=42)) # Changed ExtraTrees to RandomForest for consistency\n",
        "]\n",
        "\n",
        "meta_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(solver='saga', max_iter=5000, random_state=42))\n",
        "])\n",
        "\n",
        "stacked_model = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=meta_model,\n",
        "    cv=10,\n",
        "    n_jobs=-1,\n",
        "    passthrough=True  # Pass original features to meta-model\n",
        ")\n",
        "\n",
        "# Parameter grid for RandomizedSearch\n",
        "param_dist = {\n",
        "    'rf__n_estimators': randint(100, 300),\n",
        "    'rf__max_depth': randint(5, 15),\n",
        "    'xgb__xgb_model__n_estimators': randint(50, 200), # Adjusted param name for pipeline\n",
        "    'xgb__xgb_model__max_depth': randint(3, 10), # Adjusted param name for pipeline\n",
        "    'xgb__xgb_model__learning_rate': uniform(0.01, 0.3), # Adjusted param name for pipeline\n",
        "    'svm__svc__C': uniform(0.1, 10),\n",
        "    'et__n_estimators': randint(100, 300),\n",
        "    'et__max_depth': randint(5, 15),\n",
        "    'final_estimator__lr__C': uniform(0.1, 10),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=stacked_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", search.best_params_)\n",
        "print(\"Best CV Accuracy:\", search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = search.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
